{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ADM HW3 Group 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import functions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#download the stop words\n",
    "nltk.download(\"stopwords\")\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'C:\\\\Users\\\\elena\\\\Documents\\\\DataScience\\\\ADM_18\\\\HW_3\\\\HW3\\\\functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Upload data \n",
    "## 2: Create documents\n",
    "First mission was to split and write our dataset into tab separated files for each line. To do this we:\n",
    " - create a dataframe with our data\n",
    " - removed new lines (\\n)\n",
    " - transformed df in a list\n",
    " - create a tsv file with \\t tab space for each row from the df1 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv as data frame with pandas\n",
    "df = pd.read_csv(\"data/Airbnb_Texas_Rentals.csv\")\n",
    "\n",
    "#remove \\\\n new line from the description\n",
    "df = df.replace(r'\\\\n', \" \", regex = True)\n",
    "\n",
    "#make the df as list\n",
    "df1 = df.values.tolist()\n",
    "#remove the index from df1\n",
    "for i in df1:\n",
    "    i.remove(i[0])\n",
    "#create a tsv file with \\t tab space for each row from the df1 list\n",
    "\n",
    "#for row in df1:\n",
    "    #with open(\"doc_tsv/doc_\" +str(df1.index(row))+ \".tsv\", 'w', newline='',encoding='utf8') as f_output:\n",
    "       # tsv_output = csv.writer(f_output, delimiter='\\t',)\n",
    "        #tsv_output.writerow(row[1:]) #remove first row with names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Search Engine\n",
    "#### All the data frame was preprocessed by\n",
    "\n",
    "1) Removing stop words\n",
    "\n",
    "2) Removing punctuation\n",
    "\n",
    "3) Stemming\n",
    "\n",
    "4) Tokenization\n",
    "\n",
    "To make our computational steps faster we applied the preprocessing only to description and title columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english') # get english stop words\n",
    "#we tokenize except for the symbol $ that remains with the value\n",
    "tokenizer = RegexpTokenizer(\"[\\w'\\$]+\")\n",
    "#stemming\n",
    "st = PorterStemmer()\n",
    "\n",
    "#tokenize description and title\n",
    "df.description = df.description.apply(str).apply(tokenizer.tokenize)\n",
    "df.title = df.title.apply(str).apply(tokenizer.tokenize)\n",
    "\n",
    "#remove stop word and punctuation from description and title\n",
    "df.description = df.description.apply(lambda x : [item for item in x if item not in stopwords])\n",
    "df.title = df.title.apply(lambda x : [item for item in x if item not in stopwords])\n",
    "\n",
    "#remove stemmings from descr and title\n",
    "df.description = df.description.apply(lambda x : [st.stem(y) for y in x])\n",
    "df.title = df.title.apply(lambda x : [st.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Conjunctive query\n",
    "\n",
    "### 3.1.1 Create your index!\n",
    "\n",
    " To create an inverted index, we first split the content field of each document into separate words, create a sorted list of all the unique terms, and the list in which documents each term appears. Before building the index, we created a file named vocabulary, that maps each word to an integer (term_id). Then we created our inverted index.\n",
    "\n",
    "We decide to store vocabulary, index and inverted index into JSON files to let them easily accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the list of all the words\n",
    "voc = []\n",
    "\n",
    "for i in list(df.description):\n",
    "    for j in i:\n",
    "        voc.append(j)\n",
    "for i in list(df.title):\n",
    "    for j in i:\n",
    "        voc.append(j)\n",
    "voc = set(voc) #convert the list to a set of distinct words\n",
    "num = list(voc) #make voc as list\n",
    "vocabulary = {} #create empty dictionary\n",
    "\n",
    "for i in num:\n",
    "    vocabulary.update({num.index(i):i}) # get a key for each value of voc\n",
    "\n",
    "#we save the vocabulary as json file\n",
    "with open(\"vocabulary.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed the presence of some words in oriental characters (logograms). We decide to not remove them because since we are representing real data, it can not be excluded that the query or the websites are in another language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict from df that assign for each rows (doc) all the words in\n",
    "newdict = {}\n",
    "for index, row in df.iterrows():\n",
    "    newdict[\"doc_\" + str(index)] = list(row[\"description\"]) + list(row[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.json\", \"w\", encoding = \"utf8\") as myfile:#write normal index to json\n",
    "    myfile.write(json.dumps(newdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inverted index called inverse\n",
    "\n",
    "inverse = dict() \n",
    "for key in newdict: \n",
    "    # Go through the list that is saved in the dict:\n",
    "    for item in set(newdict[key]):\n",
    "        # Check if in the inverted dict the key exists\n",
    "        if item not in inverse: \n",
    "            # If not create a new list\n",
    "            inverse[item] = [key] \n",
    "        else: \n",
    "            inverse[item].append(key) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the keys of inverse with the keys of vocabulary\n",
    "for i in vocabulary:\n",
    "    inverse[i] = inverse[vocabulary[i]]\n",
    "    del inverse[vocabulary[i]]\n",
    "#save inverted index as json\n",
    "with open(\"inverted_index.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(inverse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Execute the query\n",
    "##### What documents do we want?\n",
    "The final output of the query its return the following information for each of the selected documents:\n",
    "\n",
    "* Title\n",
    "* Description\n",
    "* City\n",
    "* Clickable Url \n",
    "\n",
    "Non-existing queries its generate a warning message: \"result for your query doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query: pergola\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_782f22b6_f0fa_11e8_882e_3cf862c2e150\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Title</th> \n",
       "        <th class=\"col_heading level0 col1\" >Description</th> \n",
       "        <th class=\"col_heading level0 col2\" >City</th> \n",
       "        <th class=\"col_heading level0 col3\" >URL</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_782f22b6_f0fa_11e8_882e_3cf862c2e150level0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_782f22b6_f0fa_11e8_882e_3cf862c2e150row0_col0\" class=\"data row0 col0\" >Relaxing Pool in NASA Area</td> \n",
       "        <td id=\"T_782f22b6_f0fa_11e8_882e_3cf862c2e150row0_col1\" class=\"data row0 col1\" >My place is close to JSC NASA Space Center, Clear Lake University of Houston, Galveston, Kemah Boardwalk, Bay Brook Mall, 30 minutes from Hobby Airport, family-friendlylove my place because it offers a nice pool and a backyard to relax. My place is good for couples, solo adventurers, business travelers, and furry friends (pets).</td> \n",
       "        <td id=\"T_782f22b6_f0fa_11e8_882e_3cf862c2e150row0_col2\" class=\"data row0 col2\" >Houston</td> \n",
       "        <td id=\"T_782f22b6_f0fa_11e8_882e_3cf862c2e150row0_col3\" class=\"data row0 col3\" ><a target=\"_blank\" href=\"https://www.airbnb.com/rooms/13867449?location=Beach%20City%2C%20TX\n",
       "\">https://www.airbnb.com/rooms/13867449?location=Beach%20City%2C%20TX\n",
       "</a></td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x28caec60048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try \"garden pergola\":\n",
    "functions.searchEngine_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have already said at the beginning, our query with no english words can produces a result.\n",
    "# es 别墅二层独立房间 as input\n",
    "functions.searchEngine_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for a non-existent word as \"abcdefg\"\n",
    "functions.searchEngine_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Conjunctive query & Ranking score\n",
    "\n",
    "### 3.2.1 Inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist = [x for x in vocabulary.keys()] #list of words\n",
    "corpus = json.loads(open('index.json').read())\n",
    "inverted_index = json.loads(open('inverted_index.json').read())\n",
    "vocab = json.loads(open('vocabulary.json').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need now is to calculate che IDF and TF - IDF, according to the formulas: \n",
    "- $TF = \\frac{N_{(x,y)}}{N_{(*,y)}}$\n",
    "- $IDF = log[1 + (\\frac{D}{D_x})]$ <fr>\n",
    "\n",
    "Where:\n",
    "- $N_{(x,y)}$ is the number of times that the word $X$ is in the document $D_y$;\n",
    "- $N_{(*,y)}$ is the total number of the words in the document;\n",
    "- $D$ is the total number of documents;\n",
    "- $D_x$ is the number of documents in which the word $X$ appears at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the IDF\n",
    "idf = {}\n",
    "for term in inverted_index:\n",
    "    term1 = vocab[term]\n",
    "    idf[term] = math.log(1+(len(corpus)/len(term1)))\n",
    "with open(\"idf_index.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the TF - IDF\n",
    "for term in inverted_index:\n",
    "    term1 = vocab[term]\n",
    "    for doc in inverted_index[term]:\n",
    "        count = (corpus[doc].count(term1)/len(corpus[doc]))*idf[term]\n",
    "        inverted_index[term][inverted_index[term].index(doc)]=(doc,count)\n",
    "        \n",
    "with open(\"tfidf_index.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(inverted_index))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Execute the query - Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.searchEngine_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define a new score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"price\"] = df[\"average_rate_per_night\"].replace({'\\$':''}, regex = True)\n",
    "df[\"price\"] = df[\"price\"].replace(np.nan, '0', regex=True)\n",
    "\n",
    "df.price = df.price.astype(\"int\") #make the price as int\n",
    "\n",
    "\n",
    "df[\"bedroom\"] = df[\"bedrooms_count\"].replace({'Studio':'1'}, regex=True)\n",
    "df[\"bedroom\"] = df[\"bedroom\"].replace(np.nan, '0', regex=True)\n",
    "df.bedroom = df.bedroom.astype(\"int\")\n",
    "\n",
    "df[\"price_score\"] = abs((df[\"price\"] - df[\"price\"].max() )/(df[\"price\"].max() - df[\"price\"].min()) )\n",
    "df[\"bed_score\"] = (df[\"bedroom\"] - df[\"bedroom\"].min() )/(df[\"bedroom\"].max() - df[\"bedroom\"].min())\n",
    "\n",
    "df[\"av_score\"] = (df[\"price_score\"] + df[\"bed_score\"])/2\n",
    "\n",
    "av_price_city = df.groupby(\"city\")[\"price\"].agg(['mean','count','max','min'])\n",
    "\n",
    "av_price_city[\"min_int\"] = (av_price_city[\"mean\"] - av_price_city[\"min\"])/3\n",
    "av_price_city[\"max_int\"] = (av_price_city[\"max\"] - av_price_city[\"mean\"])/3\n",
    "\n",
    "newdf = pd.merge(df, av_price_city, on=\"city\")\n",
    "\n",
    "newdf['city_score'] = newdf.apply(city_score, axis=1)\n",
    "\n",
    "newdf[\"city_score\"] = (newdf[\"city_score\"] - newdf[\"city_score\"].min() )/(newdf[\"city_score\"].max() - newdf[\"city_score\"].min())\n",
    "\n",
    "newdf.loc[newdf['count'] < 5, 'city_score'] = 0.5 \n",
    "\n",
    "newdf=newdf.drop(['mean','count','max','min','min_int','max_int', \"av_score\"],axis=1)\n",
    "\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[\"new_score\"] = (newdf[\"price_score\"] + newdf[\"bed_score\"] + newdf[\"city_score\"])/3\n",
    "\n",
    "newdf.head()\n",
    "\n",
    "scores = pd.DataFrame(columns = [\"doc\", \"av_score\"])\n",
    "\n",
    "scores[\"doc\"] = newdf.index\n",
    "\n",
    "scores[\"av_score\"]=newdf.new_score\n",
    "\n",
    "\n",
    "\n",
    "newlist=scores.values.tolist()\n",
    "\n",
    "scores_dict = {}\n",
    "\n",
    "for i in newlist:\n",
    "    scores_dict[\"doc_\"+str(int(i[0]))]=i[1]\n",
    "with open(\"scores_dict.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(scores_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.searchEngine_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FOR BONUS STEP LOOK AT MAPS.IPYNB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
